{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data from the API.\n",
    "listings_json = requests.get(\"http://127.0.0.1:5000/housingDataAPI/v1.0/listings\").json()\n",
    "\n",
    "# Examine the data.\n",
    "print(json.dumps(listings_json[0], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to use for our model.\n",
    "housing_data = pd.DataFrame(listings_json)\n",
    "\n",
    "print(len(housing_data))\n",
    "housing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify home types \n",
    "for i in housing_data.index:\n",
    "    if \"Floating\" in housing_data.at[i, \"home_type\"]:\n",
    "        housing_data.at[i, \"home_type\"] = \"Floating\"\n",
    "    if \"Condo\" in housing_data.at[i, \"home_type\"]:\n",
    "        housing_data.at[i, \"home_type\"] = \"Condo\"\n",
    "    if \"Single Family\" in housing_data.at[i, \"home_type\"]:\n",
    "        housing_data.at[i, \"home_type\"] = \"Single Family\"\n",
    "    if \"Manufactured\" in housing_data.at[i, \"home_type\"]:\n",
    "        housing_data.at[i, \"home_type\"] = \"Manufactured\"\n",
    "    \n",
    "housing_data.home_type.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data to compare how many data points lost\n",
    "print(f'Current Amount of Listings: {len(housing_data)}')\n",
    "\n",
    "# Change lot size to 0 for floating homes and condos\n",
    "for i in housing_data.index:\n",
    "    if housing_data.at[i, \"home_type\"] == \"Floating\":\n",
    "        housing_data.at[i, \"lot_size\"] = 0\n",
    "    if housing_data.at[i, \"home_type\"] == \"Condo\":\n",
    "        housing_data.at[i, \"lot_size\"] = 0\n",
    "\n",
    "# Drop listing with null lot_size\n",
    "cleaned_housing_data = housing_data.drop(housing_data[housing_data[\"lot_size\"].isnull()].index)\n",
    "      \n",
    "# Print length of data\n",
    "print(f'Updated Amount of Listings: {len(cleaned_housing_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop listings with unclear Highschool data\n",
    "cleaned_housing_data.drop(cleaned_housing_data[cleaned_housing_data.high_school == \"Current Price:\"].index, inplace = True)\n",
    "cleaned_housing_data.drop(cleaned_housing_data[cleaned_housing_data.high_school == \"Other\"].index, inplace = True)\n",
    "cleaned_housing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cost ranker based on zipcode\n",
    "zipcode = cleaned_housing_data[[\"price\",\"zipcode\"]]\n",
    "zipcodeAVG = zipcode.groupby([\"zipcode\"]).mean().sort_values(by=[\"price\"], ascending=False)\n",
    "zipcodeRanker = zipcodeAVG.reset_index(drop=False)\n",
    "zipcodeRanker.reset_index(drop=False, inplace=True)\n",
    "zipcodeRanker.rename(columns={\"index\":\"zipcode_rank\",\"price\":\"zipcodeAVGcost\"}, inplace=True)\n",
    "zipcodeRanker[\"zipcode_rank\"]=zipcodeRanker[\"zipcode_rank\"]+1\n",
    "\n",
    "# Merge into df\n",
    "cleaned_housing_data2 = pd.merge(cleaned_housing_data, zipcodeRanker, on=\"zipcode\")\n",
    "cleaned_housing_data2.rename(columns={\"price_y\":\"zipcodeAVGcost\"}, inplace = True)\n",
    "cleaned_housing_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create district df\n",
    "school_dict = ({\"high_school\" : ['Reynolds', 'Parkrose', 'David Douglas', 'Centennial', 'Cleveland',\n",
    "        'Lincoln', 'Madison', 'Jefferson', 'Roosevelt', 'Sunset','Westview', 'Liberty', 'Beaverton', \n",
    "        'Grant', 'Southridge', 'Tigard', 'Wilson', 'Riverdale', 'Lake Oswego', 'Franklin',\n",
    "        'Tualatin', 'Milwaukie', 'Scappoose'], \"district\" : ['Reynolds', 'Parkrose','David Douglas',\n",
    "        'Centennial', 'Portland Public', 'Portland Public', 'Portland Public', 'Portland Public',\n",
    "        'Portland Public', 'Beaverton', 'Beaverton', 'Hillsboro', 'Beaverton', 'Portland Public',\n",
    "        'Beaverton', 'Tigard-Tualatin', 'Portland Public', 'Riverdale', 'Lake Oswego', 'Portland Public',\n",
    "        'Tigard-Tualatin', 'North Clackamas', 'Scappose']})\n",
    "district_df = pd.DataFrame (school_dict)\n",
    "\n",
    "# Merge into OG df\n",
    "cleaned_housing_data3 = pd.merge(cleaned_housing_data2, district_df, on=\"high_school\")\n",
    "cleaned_housing_data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cost ranker based on high schools\n",
    "hs = cleaned_housing_data3[[\"price\",\"high_school\"]]\n",
    "hsAVG = hs.groupby([\"high_school\"]).mean().sort_values(by=[\"price\"], ascending=False)\n",
    "hsRanker = hsAVG.reset_index(drop=False)\n",
    "hsRanker.reset_index(drop=False, inplace=True)\n",
    "hsRanker.rename(columns={\"index\":\"hs_rank\",\"price\":\"hsAVGcost\"}, inplace=True)\n",
    "hsRanker[\"hs_rank\"]= hsRanker[\"hs_rank\"]+1\n",
    "\n",
    "# Create a cost ranker based on districts\n",
    "district = cleaned_housing_data3[[\"price\",\"district\"]]\n",
    "districtAVG = district.groupby([\"district\"]).mean().sort_values(by=[\"price\"], ascending=False)\n",
    "districtRanker = districtAVG.reset_index(drop=False)\n",
    "districtRanker.reset_index(drop=False, inplace=True)\n",
    "districtRanker.rename(columns={\"index\":\"district_rank\",\"price\":\"districtAVGcost\"}, inplace=True)\n",
    "districtRanker[\"district_rank\"]= districtRanker[\"district_rank\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge high school and district rankers \n",
    "cleaned_housing_data4 = pd.merge(cleaned_housing_data3, hsRanker, on=\"high_school\")\n",
    "cleaned_housing_data_5 = pd.merge(cleaned_housing_data4, districtRanker, on=\"district\")\n",
    "cleaned_housing_data_final = cleaned_housing_data_5[['address', 'price', 'home_type', 'bedrooms', \n",
    "                                'bathrooms', 'square_feet', 'built', 'lot_size', 'neighborhood', \n",
    "                                'county', 'city', 'zipcode', 'zipcode_rank', 'zipcodeAVGcost',\n",
    "                                'elementary_school', 'middle_school', 'high_school','hs_rank', \n",
    "                                'hsAVGcost', 'district', 'district_rank', 'districtAVGcost']]\n",
    "\n",
    "cleaned_housing_data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original data frame to modify.\n",
    "model_df = cleaned_housing_data_final\n",
    "\n",
    "# Include only those columns that will be used in the deep learning model.\n",
    "model_df = model_df.loc[:, [\"bathrooms\",\n",
    "                            \"bedrooms\",\n",
    "                            \"built\",\n",
    "                            \"lot_size\",\n",
    "                            \"square_feet\",\n",
    "#                             \"neighborhood\",\n",
    "#                             \"county\",\n",
    "#                             \"home_type\",\n",
    "#                             \"hs_rank\",\n",
    "#                             \"hsAVGcost\",\n",
    "                            \"districtAVGcost\",\n",
    "#                             \"district_rank\",\n",
    "                            \"zipcodeAVGcost\",\n",
    "#                             \"zipcode_rank\",\n",
    "                            \"price\"]\n",
    "                       ]\n",
    "\n",
    "# Drop rows with NaN entries.\n",
    "model_df.dropna(inplace=True)\n",
    "\n",
    "# Check the model data.\n",
    "print(len(model_df))\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin prices into ten equal length ranges.\n",
    "model_df[\"price_range\"] = pd.qcut(model_df[\"price\"], 5)\n",
    "# Drop the original price data.\n",
    "model_df.drop(\"price\", axis=1, inplace=True)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get dummies for the values in home_type to use in the model.\n",
    "# model_df = pd.get_dummies(model_df, columns=[\"home_type\"])\n",
    "# model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X (input) and y (target).\n",
    "X = model_df.drop(\"price_range\", axis=1)\n",
    "y = model_df[\"price_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler model and fit it to the training data\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# Save the scalar.\n",
    "dump(X_scaler, open(\"minmax_scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler and y_scaler models.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target data.\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Save the label encoder\n",
    "dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "print(y_train[0])\n",
    "print(encoded_y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded labels to one-hot encoding.\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)\n",
    "y_train_categorical[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier, fit to the training data, and score on the testing data.\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf = rf.fit(X_train_scaled, y_train_categorical)\n",
    "print(rf.score(X_test_scaled, y_test_categorical))\n",
    "\n",
    "# Find the importances of each feature.\n",
    "feature_names = X.columns\n",
    "importances = rf.feature_importances_\n",
    "print(sorted(zip(rf.feature_importances_, feature_names), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep learning Sequential model.\n",
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(units=500, activation='relu', input_dim=7))\n",
    "deep_model.add(Dense(units=200, activation='relu'))\n",
    "deep_model.add(Dense(units=100, activation='relu'))\n",
    "deep_model.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the model.\n",
    "deep_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "deep_model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = deep_model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f97eb3e97245187b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the first 10 test data values to make a prediction and compare it to the actual labels.\n",
    "encoded_predictions = deep_model.predict_classes(X_test_scaled[:10])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)\n",
    "\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# deep_model.save(\"housing_model_trained.h5\")\n",
    "\n",
    "def unpack(model, training_config, weights):\n",
    "    restored_model = deserialize(model)\n",
    "    if training_config is not None:\n",
    "        restored_model.compile(\n",
    "            **saving_utils.compile_args_from_training_config(\n",
    "                training_config\n",
    "            )\n",
    "        )\n",
    "    restored_model.set_weights(weights)\n",
    "    return restored_model\n",
    "\n",
    "# Hotfix function\n",
    "def make_keras_picklable():\n",
    "\n",
    "    def __reduce__(self):\n",
    "        model_metadata = saving_utils.model_metadata(self)\n",
    "        training_config = model_metadata.get(\"training_config\", None)\n",
    "        model = serialize(self)\n",
    "        weights = self.get_weights()\n",
    "        return (unpack, (model, training_config, weights))\n",
    "\n",
    "    cls = Model\n",
    "    cls.__reduce__ = __reduce__\n",
    "\n",
    "# Run the function\n",
    "make_keras_picklable()\n",
    "\n",
    "dump(deep_model, open(\"housing_model_trained.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the saved model, scaler, and label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(model, training_config, weights):\n",
    "    restored_model = deserialize(model)\n",
    "    if training_config is not None:\n",
    "        restored_model.compile(\n",
    "            **saving_utils.compile_args_from_training_config(\n",
    "                training_config\n",
    "            )\n",
    "        )\n",
    "    restored_model.set_weights(weights)\n",
    "    return restored_model\n",
    "\n",
    "# Hotfix function\n",
    "def make_keras_picklable():\n",
    "\n",
    "    def __reduce__(self):\n",
    "        model_metadata = saving_utils.model_metadata(self)\n",
    "        training_config = model_metadata.get(\"training_config\", None)\n",
    "        model = serialize(self)\n",
    "        weights = self.get_weights()\n",
    "        return (unpack, (model, training_config, weights))\n",
    "\n",
    "    cls = Model\n",
    "    cls.__reduce__ = __reduce__\n",
    "\n",
    "# Run the function\n",
    "make_keras_picklable()\n",
    "\n",
    "# Load the model, scaler and label encoder.\n",
    "scaler = load(open(\"minmax_scaler.pkl\", \"rb\"))\n",
    "label_encoder = load(open(\"label_encoder.pkl\", \"rb\"))\n",
    "model = load(open(\"housing_model_trained.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data for testing.\n",
    "input_data = np.array(np.array([[2.5, 4, 1920, 0.5, 1000, 653295.8659180978, 843569.7441860465]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349500.0, 449000.0\n"
     ]
    }
   ],
   "source": [
    "encoded_predictions = model.predict_classes(scaler.transform(input_data))\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)\n",
    "\n",
    "print(f\"{prediction_labels[0].left}, {prediction_labels[0].right}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
