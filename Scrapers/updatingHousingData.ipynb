{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser \n",
    "import requests\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import MetaData, Table, Column, Integer, String, Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Splinter\n",
    "executable_path = {'executable_path': '/Users/daniellove/Downloads/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "# Directing splinter to webpage\n",
    "portlandMLS_url = 'https://www.portlandmlsdirect.com/cgi-bin/real?pge=newsearch&state=na&widget=true&sortby=price&area=Portland+%28City%29&price_lo=0&price_hi=100000000&tot_bed_lo=0&tot_bath_lo=0&htype=ALL'\n",
    "browser.visit(portlandMLS_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables\n",
    "tracker = 0\n",
    "updated_links = []\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soupy = BeautifulSoup(browser.html, 'html.parser')\n",
    "\n",
    "# Identify the number of pages of houses\n",
    "number_of_pages = soupy.find(id='ia_btn_text').text\n",
    "page_count = int(number_of_pages[5:])\n",
    "print(page_count)\n",
    "\n",
    "# Look going through all the pages of listings\n",
    "for p in range(1, page_count):\n",
    "    tracker += 1 \n",
    "    \n",
    "    # Identify all items in the gridview class\n",
    "    viewgrid_all = soupy.find_all('div', class_='viewgrid')\n",
    "    last_item = viewgrid_all[-1].get('id')\n",
    "    viewgrid_count = int(last_item[8:]) + 1\n",
    "    \n",
    "    # Loop going through each home listing on a page\n",
    "    for g in range(1, viewgrid_count):\n",
    "        \n",
    "        # Create BeautifulSoup object\n",
    "        soupy = BeautifulSoup(browser.html, 'html.parser')\n",
    "        \n",
    "        # Click on each grid item\n",
    "        elems = soupy.find('div', {'id':(f'viewgrid{g}')}).find('a')\n",
    "        start_html = \"https://www.portlandmlsdirect.com/\"  \n",
    "        link = start_html + elems['href']\n",
    "        updated_links.append(link)\n",
    "    \n",
    "    # Go to next page\n",
    "    browser.find_by_id(\"ia_btn_next\").click()\n",
    "    print(f'Completion Percentage: {round((tracker/page_count)*100, 1)}%')\n",
    "    \n",
    "print(len(updated_links))\n",
    "print(updated_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of links of the data previously scrape\n",
    "with open(\"../Resources/housing_links.txt\", \"rb\") as fp:   # Unpickling\n",
    "    link_list = pickle.load(fp)\n",
    "    \n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables\n",
    "new_listings = []\n",
    "\n",
    "# Find new listings \n",
    "for link in updated_links:\n",
    "    if link in link_list:\n",
    "        pass\n",
    "    else:\n",
    "        new_listings.append(link)\n",
    "        link_list.append(link)\n",
    "        \n",
    "print(new_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "list_home_dict = []\n",
    "loading_error_links = []\n",
    "error_count = 0\n",
    "bath = \"BATH\"\n",
    "acres = \"ACRES\"\n",
    "neighbor = \"Neighborhood\"\n",
    "\n",
    "# Loop to iterate through all listing links\n",
    "for links in new_listings: \n",
    "    try:\n",
    "        # Navigate to webpages for each home\n",
    "        browser.visit(links)\n",
    "        soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "\n",
    "        # To categorize homes with 0 bedrooms\n",
    "        if bath in soup.find_all('div', class_=\"lineitem\")[0].text:\n",
    "            # Scraping primary data \n",
    "            address = soup.find('div', id=\"ia_address\").text.replace(\"\\n\",\"\").replace(\"\\t  \", \"\")\n",
    "            price = int(soup.find('div', id=\"ia_price\").text.strip().replace('$', '').replace(',',''))\n",
    "            bedrooms = 0 \n",
    "            bathrooms = int(soup.find_all('div', class_=\"lineitem\")[0].text[0])\n",
    "            square_feet = int(soup.find_all('div', class_=\"lineitem\")[1].text.replace(\n",
    "                                                            'SQFT', '').replace(',', ''))\n",
    "        \n",
    "        else:\n",
    "            # Scraping primary data\n",
    "            address = soup.find('div', id=\"ia_address\").text.replace(\"\\n\",\"\").replace(\"\\t  \", \"\")\n",
    "            price = int(soup.find('div', id=\"ia_price\").text.strip().replace('$', '').replace(',',''))\n",
    "            bedrooms = int(soup.find_all('div', class_=\"lineitem\")[0].text[0])\n",
    "            bathrooms = float(soup.find_all('div', class_=\"lineitem\")[1].text[0])\n",
    "            square_feet = int(soup.find_all('div', class_=\"lineitem\")[2].text.replace(\n",
    "                                                            'SQFT', '').replace(',', ''))\n",
    "        \n",
    "        home_type = soup.find_all('div', id=\"PropDetailItem\")[5].text[12:].replace('\\n', '')\n",
    "        built = int(soup.find_all('div', id=\"PropDetailItem\")[3].text[12:])\n",
    "        \n",
    "        # Deals with pages that don't mention lot size\n",
    "        if acres in soup.find_all('div', class_=\"lineitem\")[4].text:\n",
    "            lot_size = float(soup.find_all('div', class_=\"lineitem\")[4].text.replace('ACRES', ''))\n",
    "\n",
    "        else:\n",
    "            lot_size = np.NaN \n",
    "\n",
    "        # Deals with pages that don't mention neighborhood\n",
    "        if neighbor == soup.find_all('div', id='areaitemTitle')[3].text:\n",
    "            neighborhood = soup.find_all('div', id='areaitemValue')[3].text\n",
    "\n",
    "        else:\n",
    "            neighborhood = \"unknown\"\n",
    "            \n",
    "        county = soup.find_all('div', id='areaitemValue')[0].text\n",
    "        city = soup.find_all('div', id='areaitemValue')[1].text\n",
    "        zipcode = soup.find_all('div', id='areaitemValue')[2].text\n",
    "\n",
    "        # Schools data\n",
    "        HS = soup.find_all('div', id=\"PropDetailTitle\")[0].find_all_next('div')[10].text\n",
    "        MS = soup.find_all('div', id=\"PropDetailTitle\")[0].find_all_next('div')[6].text\n",
    "        ES = soup.find_all('div', id=\"PropDetailTitle\")[0].find_all_next('div')[2].text\n",
    "\n",
    "        # Create dictionary to hold data collected\n",
    "        home_dict = {\n",
    "                'address':address,\n",
    "                'price':price,\n",
    "                'home_type':home_type,\n",
    "                'bedrooms':bedrooms,\n",
    "                'bathrooms':bathrooms,\n",
    "                'square_feet':square_feet,\n",
    "                'built':built,\n",
    "                'lot_size':lot_size,\n",
    "                'neighborhood':neighborhood,\n",
    "                'county':county,\n",
    "                'city':city,\n",
    "                'zipcode':zipcode,\n",
    "                'high_school':HS,\n",
    "                'middle_school':MS,\n",
    "                'elementary_school':ES\n",
    "            }\n",
    "        print(home_dict)\n",
    "\n",
    "        # Append dictionaries to a list\n",
    "        list_home_dict.append(home_dict)\n",
    "    \n",
    "    # Handles errors and collect links which data was able to be pulled\n",
    "    except Exception as e:\n",
    "        print('-------------')\n",
    "        loading_error_links.append(links)\n",
    "        error_count += 1\n",
    "        print(error_count, e)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df \n",
    "housing_data_df = pd.DataFrame(list_home_dict)\n",
    "\n",
    "# Combine new data, drop duplicates\n",
    "scraped_data = pd.read_csv(\"../Resources/housingDataUpdated.csv\")\n",
    "data_combined = scraped_data.append(housing_data_df)\n",
    "data_combined.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop duplicates and save housing data\n",
    "data_combined.to_csv(\"../Resources/housingDataUpdated.csv\", index = False, header = True)\n",
    "\n",
    "# Save updated list of links\n",
    "with open(\"../Resources/housing_linksUpdated.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(link_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database connection.\n",
    "database_path = \"../Resources/housingUpdated.sqlite\"\n",
    "engine = create_engine(f\"sqlite:///{database_path}\")\n",
    "meta = MetaData(engine)\n",
    "conn = engine.connect()\n",
    "\n",
    "# Drop all current data.\n",
    "meta.drop_all()\n",
    "\n",
    "# Create the listings table.\n",
    "listings = Table(\n",
    "    \"listings\", meta,\n",
    "    Column(\"id\", Integer, primary_key=True),\n",
    "    Column(\"address\", String(255)),\n",
    "    Column(\"price\", Integer),\n",
    "    Column(\"home_type\", String(255)),\n",
    "    Column(\"bedrooms\", Integer),\n",
    "    Column(\"bathrooms\", Float),\n",
    "    Column(\"square_feet\", Integer),\n",
    "    Column(\"built\", Integer),\n",
    "    Column(\"lot_size\", Float),\n",
    "    Column(\"neighborhood\", String(255)),\n",
    "    Column(\"county\", String(255)),\n",
    "    Column(\"city\", String(255)),\n",
    "    Column(\"zipcode\", Integer),\n",
    "    Column(\"high_school\", String(255)),\n",
    "    Column(\"middle_school\", String(255)),\n",
    "    Column(\"elementary_school\", String(255))\n",
    ")\n",
    "meta.create_all()\n",
    "\n",
    "# Insert data into the database.\n",
    "for index, row in data_combined.iterrows():\n",
    "    ins = listings.insert().values(\n",
    "      address = row[\"address\"],\n",
    "      price = row[\"price\"],\n",
    "      home_type = row[\"home_type\"],\n",
    "      bedrooms = row[\"bedrooms\"],\n",
    "      bathrooms = row[\"bathrooms\"],\n",
    "      square_feet = row[\"square_feet\"],\n",
    "      built = row[\"built\"],\n",
    "      lot_size = row[\"lot_size\"],\n",
    "      neighborhood = row[\"neighborhood\"],\n",
    "      county = row[\"county\"],\n",
    "      city = row[\"city\"],\n",
    "      zipcode = row[\"zipcode\"],\n",
    "      high_school = row[\"high_school\"],\n",
    "      middle_school = row[\"middle_school\"],\n",
    "      elementary_school = row[\"elementary_school\"]\n",
    "      )\n",
    "    conn.execute(ins)\n",
    "\n",
    "# Close the connection.\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
